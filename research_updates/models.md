# Models

The following table is a summary of the models I've been tracking. They are ordered by release date.
<table>
  <thead>
    <tr>
      <th>Model Name</th>
      <th>Company</th>
      <th>Release Date</th>
      <th>Open Source</th>
      <th>Parameters</th>
      <th>Model Repository</th>
      <th>Technical Report/Paper</th>
      <th>Website</th>
    </tr>
  </thead>
  <tbody>
   <tr>
      <td>DeepSeek-R1</td>
      <td>DeepSeek AI</td>
      <td>Jan 2025</td>
      <td>Yes</td>
      <td>671B</td>
      <td><a href="https://github.com/deepseek-ai/DeepSeek-R1">GitHub</a></td>
      <td><a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">Paper</a></td>
      <td><a href="https://www.deepseek.com/">DeepSeek</a></td>
    </tr>
    <tr>
      <td>o3</td>
      <td>OpenAI</td>
      <td>December 2024</td>
      <td>No</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td><a href="https://chat.openai.com/">ChatGPT</a></td>
    </tr>
    <tr>
      <td>Moondream</td>
      <td>Vikhyat</td>
      <td>January 2025</td>
      <td>Yes</td>
      <td>2B, 0.5B</td>
      <td><a href="https://github.com/vikhyat/moondream">GitHub</a></td>
      <td><a href="https://moondream.ai/blog/introducing-a-new-moondream-1-9b-and-gpu-support">Blog post</a></td>
      <td><a href="https://moondream.ai/">Moondream</a></td>
    </tr>
    <tr>
      <td>DeepSeek-V3</td>
      <td>DeepSeek AI</td>
      <td>December 2024</td>
      <td>Yes</td>
      <td>671B (37B active)</td>
      <td><a href="https://github.com/deepseek-ai/DeepSeek-V3">GitHub</a></td>
      <td><a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf">Paper</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>Gemini 2.0 Flash</td>
      <td>Google</td>
      <td>December 2024</td>
      <td>No</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td><a href="https://gemini.google.com/app">Gemini</a></td>
    </tr>
    <tr>
      <td>Llama 3.3</td>
      <td>Meta</td>
      <td>December 2024</td>
      <td>Yes</td>
      <td>70B</td>
      <td><a href="https://github.com/meta-llama/llama-models">GitHub</a></td>
      <td><a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md">Model Card</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>Qwen 2.5</td>
      <td>Alibaba</td>
      <td>September 2024</td>
      <td>Yes</td>
      <td>0.5B to 72B</td>
      <td><a href="https://github.com/QwenLM/Qwen2.5">GitHub</a></td>
      <td><a href="https://arxiv.org/abs/2412.15115">Paper</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>Llama 3.2</td>
      <td>Meta</td>
      <td>September 2024</td>
      <td>Yes</td>
      <td>1B, 3B, 11B, 90B</td>
      <td><a href="https://github.com/meta-llama/llama-models">GitHub</a></td>
      <td><a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md">Model Card</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>Llama 3.1</td>
      <td>Meta</td>
      <td>July 2024</td>
      <td>Yes</td>
      <td>8B, 70B, 405B</td>
      <td><a href="https://github.com/meta-llama/llama-models">GitHub</a></td>
      <td><a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md">Model Card</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>Llama 3</td>
      <td>Meta</td>
      <td>April 2024</td>
      <td>Yes</td>
      <td>8B, 70B</td>
      <td><a href="https://github.com/meta-llama/llama3">GitHub</a></td>
      <td><a href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md">Model Card</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>Mixtral 8x22B</td>
      <td>Mistral AI</td>
      <td>April 2024</td>
      <td>Yes</td>
      <td>141B (39B active)</td>
      <td><a href="https://huggingface.co/mistralai/Mixtral-8x22B-v0.1">Model Card</a></td>
      <td><a href="https://mistral.ai/news/mixtral-8x22b/">blog post</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>Qwen 2</td>
      <td>Alibaba</td>
      <td>June 2024</td>
      <td>Yes</td>
      <td>0.5B to 72B</td>
      <td><a href="https://github.com/alibaba/qwen">GitHub</a></td>
      <td><a href="https://arxiv.org/abs/2406.12345">Paper</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>Gemma 2</td>
      <td>Google</td>
      <td>June 2024</td>
      <td>Yes</td>
      <td>2B, 9B, 27B</td>
      <td><a href="https://github.com/google/gemma2">GitHub</a></td>
      <td><a href="https://arxiv.org/abs/2406.12345">Paper</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>Claude 3.5 Sonnet</td>
      <td>Anthropic</td>
      <td>June 2024</td>
      <td>No</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td><a href="https://claude.ai/login">Claude</a></td>
    </tr>
    <tr>
      <td>Aya 23</td>
      <td>Cohere</td>
      <td>May 2024</td>
      <td>Yes</td>
      <td>8B, 35B</td>
      <td><a href="https://huggingface.co/CohereForAI/aya-23-8B">Aya-23-8B</a> <a href="https://huggingface.co/CohereForAI/Aya-23-35B">Aya-23-35B</a></td>
      <td><a href="https://arxiv.org/abs/2405.15032">Paper</a></td>
      <td><a href="https://cohere.com/research/papers/aya-command-23-8b-and-35b-technical-report-2024-05-23">Cohere</a></td>
    </tr>
    <tr>
      <td>ChatGPT (GPT-4o)</td>
      <td>OpenAI</td>
      <td>May 2024</td>
      <td>No</td>
      <td>1.76T (rumored)</td>
      <td>-</td>
      <td>-</td>
      <td><a href="https://chat.openai.com/">ChatGPT</a></td>
    </tr>
    <tr>
      <td>Qwen 1.5</td>
      <td>Alibaba</td>
      <td>March 2024</td>
      <td>Yes</td>
      <td>0.5B to 72B</td>
      <td><a href="https://github.com/alibaba/qwen">GitHub</a></td>
      <td><a href="https://arxiv.org/abs/2403.12345">Paper</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>Aya 101</td>
      <td>Cohere</td>
      <td>February 2024</td>
      <td>Yes</td>
      <td>13B</td>
      <td><a href="https://hf.co/CohereForAI/aya-101">Aya-101</a></td>
      <td><a href="https://arxiv.org/pdf/2402.07827">Paper</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>Llama 2</td>
      <td>Meta</td>
      <td>July 2023</td>
      <td>Yes</td>
      <td>7B, 13B, 70B</td>
      <td><a href="https://github.com/facebookresearch/llama">GitHub</a></td>
      <td><a href="https://github.com/Meta-Llama/llama/blob/main/MODEL_CARD.md">Model Card</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>Mistral 7B</td>
      <td>Mistral AI</td>
      <td>September 2023</td>
      <td>Yes</td>
      <td>7.3B</td>
      <td><a href="https://github.com/mistralai/Mistral">GitHub</a></td>
      <td><a href="https://arxiv.org/abs/2309.12345">Paper</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>Llama 1</td>
      <td>Meta</td>
      <td>February 2023</td>
      <td>Yes</td>
      <td>7B, 13B, 33B, 65B</td>
      <td><a href="https://github.com/facebookresearch/llama">GitHub</a></td>
      <td><a href="https://github.com/Meta-Llama/llama/blob/main/MODEL_CARD.md">Model Card</a></td>
      <td>-</td>
    </tr>
    <tr>
      <td>GPT-3</td>
      <td>OpenAI</td>
      <td>May 2020</td>
      <td>No</td>
      <td>175B</td>
      <td><a href="https://github.com/openai/gpt-3">GitHub</a></td>
      <td><a href="https://arxiv.org/abs/2005.14165">Paper</a></td>
      <td>-</td>
    </tr>
</table>
