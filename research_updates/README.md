# Research papers

* Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, by Anthropic [Apr 2022] - [source](https://arxiv.org/pdf/2204.05862)

## Multi-Agent
<details>
    <summary>
    ChatDev: Communicative Agents for Software Development by Qian et al. [Jun 2024]
    </summary>
        - [paper](https://arxiv.org/pdf/2307.07924) / [github](https://github.com/OpenBMB/ChatDev)
        - Company Overview: ChatDev is a virtual software company powered by intelligent agents.
        - Agents take on roles such as CEO, CTO, programmer, tester, reviewer, and designer.
        - Organizational Structure: Operates as a multi-agent system collaborating through specialized seminars.
        - Collaboration Tasks: Agents handle designing, coding, testing, and documenting software.
        - Mission: "Revolutionize the digital world through programming."
        - Framework Focus: Offers a user-friendly, customizable, and extendable framework.
        - Technology Basis: Built on large language models (LLMs).
        - Research Purpose: Serves as a platform to study and understand collective intelligence.
</details>



* ChatDev: Communicative Agents for Software Development by Qian et al. [Jun 2024] - [paper](https://arxiv.org/pdf/2307.07924)/[github](https://github.com/OpenBMB/ChatDev)
    - Company Overview: ChatDev is a virtual software company powered by intelligent agents.
    - Agents take on roles such as CEO, CTO, programmer, tester, reviewer, and designer.
    - Organizational Structure: Operates as a multi-agent system collaborating through specialized seminars.
    - Collaboration Tasks: Agents handle designing, coding, testing, and documenting software.
    - Mission: "Revolutionize the digital world through programming."
    - Framework Focus: Offers a user-friendly, customizable, and extendable framework.
    - Technology Basis: Built on large language models (LLMs).
    - Research Purpose: Serves as a platform to study and understand collective intelligence.

## Agent Frameworks:
- Code Generation with AlphaCodium: From Prompt Engineering to Flow
Engineering, Ridnik et al. (Jan 2024)[[paper](https://arxiv.org/pdf/2401.08500)]
- Reflexion: Language Agents with
Verbal Reinforcement Learning, Shinn et al (Oct 2023) [[paper](https://arxiv.org/pdf/2303.11366)]
- MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action, Yang et al. (March 2023) - [paper](https://arxiv.org/pdf/2303.11381)

## Evaluation
* Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena by Zheng et al [Dec 2023] - [paper](https://arxiv.org/pdf/2306.05685)
    - Motivation / contribution:
        * **Challenges in Evaluating LLM-Based Chat Assistants**: Traditional benchmarks are inadequate for assessing the broad capabilities of LLM-based chat assistants, especially in measuring human preferences.

        * **Scalability and Explainability**: Human evaluations are expensive and time-consuming. Utilizing LLMs as judges offers a scalable and explainable alternative to approximate human preferences.

        * **Alignment with Human Preferences**: There's a need to ensure that LLMs align with human preferences in open-ended tasks, such as multi-turn dialogues, which traditional benchmarks fail to assess effectively.

        * **Mitigating Biases in LLM Judgments**: The research identifies potential biases in LLM judgments, such as position, verbosity, and self-enhancement biases, and proposes solutions to mitigate them.

        * **Development of New Benchmarks**: The introduction of [MT-Bench](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge) and [Chatbot Arena](https://huggingface.co/datasets/lmsys/chatbot_arena_conversations) aims to provide platforms for evaluating the alignment between LLM judgments and human preferences.
        
* A Survey on Evaluation of Large Language Models - by YUPENG CHANG et al [Dec 2023] - [paper](https://arxiv.org/pdf/2307.03109)